---
layout: post
title: "fastTextのインストール"
categories: journal
tags: [nlp]
image:
  feature:
  teaser:
  credit:
  creditlink:
---
絶対後で忘れるので、残しておく

## fastTextのインストール
------------
[FacebookのfastTextでFastに単語の分散表現を獲得する](http://qiita.com/icoxfog417/items/42a95b279c0b7ad26589)
を参考にする 
- 各種gitのclone
~~~
git clone https://github.com/facebookresearch/fastText.git
cd fastText; make
git clone https://github.com/icoxfog417/fastTextJapaneseTutorial.git
git clone https://github.com/attardi/wikiextractor.git
~~~
<br /> 

## 日本語wikipediaのダンプデータをダウンロード 
------------
ここではqiitaの例を参考にwikipediaのデータを使ってモデルを作成します
- [wikipedia](https://dumps.wikimedia.org/jawiki/20170320/)からjawiki-20170320-pages-articles-multistream.xml.bz2をダウンロード
- wikiextractorのディレクトリを作成しそこに移動
~~~
mkdir ~/git/wikiextractor/source/
mv jawiki-20170320-pages-articles-multistream.xml.bz2 ~/git/wikiextractor/source/
~~~
<br />  

## wikiextractorでテキストを抽出
----------
~~~
cd ~/git/wikiextractor
mkdir corpus
cd ~/git/fastTextJapaneseTutorial/
python3 ~/git/wikiextractor/WikiExtractor.py -b 500M -o corpus ~/git/wikiextractor/source/jawiki-20170320-pages-articles-multistream.xml.bz2
~~~
<br />  

## 一つにまとめる,分かち書き変換 
----------
- 抽出されたデータはファイルに分割されているので一つにまとめる
~~~
cd ~/git/fastTextJapaneseTutorial/corpus/AA/
python3 parse.py ~/git/fastTextJapaneseTutorial/corpus/AA/ --concat wiki
#cat wiki_00 wiki_01 wiki_02 wiki_03 wiki_04 > wiki_ALL こちらでも
~~~
- URL、タグが入っているので除く
~~~
cd ~/git/fastTextJapaneseTutorial/corpus/
cat wiki_all.txt | grep -v "doc>" | grep -v "<doc" > wiki_all_no_url.txt
mecab -b 81920 wiki_all_no_url.txt -O wakati -o wiki_all_wakati.txt
#input-buffer overflow. The line is split. use -b #SIZE option.と言われるので-bで大きめに設定
~~~
<br />  

## fastTextで分散表現を取得
----------
~~~
cd ~/git/fastTextJapaneseTutorial/
mkdir fastText
../fastText/fasttext.exe skipgram -input corpus/wiki_all_wakati.txt -output fastText/model -dim 200
#dimは最大300まで指定できます
~~~
<br />  

## 単語モデルであれこれ
--------
fastTextJapaneseTutorialに同梱されているeval.pyで似たような単語を列挙
- 自分の環境では上手く動かなかったので、一部改造  
[#file-fasttextjapanesetutorial_eval-py-diff](https://gist.github.com/nosnosnosnos/1f034463510ec726dc109bacd584ab84#file-fasttextjapanesetutorial_eval-py-diff)

- 林檎と似ている単語を列挙
~~~
python3 eval.py --path fastText/model.vec 林檎
林檎, 1.0
椎名, 0.5740982884979903
ロックバンド・ポルノグラフィティ, 0.5488214515035502
ハラフウミ, 0.5475175265187614
ロックバンド・サンボマスター, 0.5461653546465494
一青, 0.5453177830993906
TRiCKY, 0.5442881218105926
りんご, 0.5427355781000773
センチメンタルマキアート, 0.5400386292473165
BABYS, 0.538407112785486
ニュアージュ, 0.5326498301163858
ミニアルバムリリース, 0.5315703265887215
ロックバンド・ステレオポニー, 0.5307528008278596
ファンキーモンキーベイビーズ, 0.5288628026397308
新鉄, 0.5285504094021384
キュウソネコカミ, 0.5282098042135296
サンボマスター, 0.5280660886883273
リンゴ, 0.5276278859983454
~~~

- 椎名林檎要素が強かったので 0.5スガシカオを引いてみる
~~~
#w_vec = vectors["林檎"] - (vectors["スガシカオ"] / 2 )
林檎, 0.8526160855613651
ヘスペリデス, 0.4240753167649298
桜桃, 0.42330573924244136
ディオニューソス, 0.41959761322660455
葡萄, 0.4172528478638852
リンゴ, 0.41312612288922174
りんご, 0.40358748399326555
ステュムパーリデス, 0.394155263203731
ヘルメース, 0.3931022439822802
羊毛, 0.39292586183683037
ハルピンナ, 0.39024781226518257
山羊, 0.3902183401495548
グラウコス, 0.38570545170607945
ヘーラー, 0.3851508846518612
ペルセポネー, 0.37868261738282877
おんどり, 0.3762488716806668
~~~

- 未知語に対してもベクトルを取得できます ←活用形のみ
~~~
echo "ももいろクローバーZ" |./fasttext.exe print-vectors ../fastTextJapaneseTutorial/fastText/model.bin  
~~~
<br />  


## 辞書を追加する
--------
未知語が多かったので[形態素解析のために Wikipedia とはてなキーワードからユーザー辞書を生成し利用する](http://qiita.com/ynakayama/items/388c82cbe14c65827769)
を参考に辞書を追加してみました
- keywordリストを取得
~~~
# はてなキーワード
curl -L http://d.hatena.ne.jp/images/keyword/keywordlist_furigana.csv | iconv -f euc-jp -t utf-8 > keywordlist_furigana.csv
# Wikipedia
curl -L http://dumps.wikimedia.org/jawiki/latest/jawiki-latest-all-titles-in-ns0.gz | gunzip > jawiki-latest-all-titles-in-ns0
~~~
- csv変換用のスクリプトを落として実行  
[conv.py](https://gist.github.com/nosnosnosnos/1f034463510ec726dc109bacd584ab84#file-conv-py)
~~~
python3 conv.py
#custom.csvが出来るので mecab-ipadic-2.7.0-20070801にmvする
~~~
- 辞書再作成
~~~
/usr/local/libexec/mecab/mecab-dict-index.exe -f utf8 -t utf8
make clean
make
make install
~~~

<br />  
